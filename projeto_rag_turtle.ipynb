{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf34abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codigo para gerar os dados mocados\n",
    "\n",
    "import json\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "import math\n",
    "\n",
    "output_path = \"./data/turtle_sensors.jsonl\"\n",
    "os.makedirs(\"./data\", exist_ok=True)\n",
    "\n",
    "NUM_ROWS = 10_000\n",
    "\n",
    "def generate_temperature(t):\n",
    "    minute_of_day = t % 1440  \n",
    "    daily_cycle = 0.5 * math.sin(2 * math.pi * (minute_of_day / 1440))\n",
    "\n",
    "    long_term_rise = 4 * (t / NUM_ROWS)\n",
    "\n",
    "    base_temp = random.uniform(17, 22)  # base for nights\n",
    "    temp = base_temp + daily_cycle * 10 + long_term_rise\n",
    "\n",
    "    return round(min(max(temp, 17), 34), 2)  # clamp between 17 and 34\n",
    "\n",
    "\n",
    "def generate_movement(t):\n",
    "    if t < NUM_ROWS * 0.75:\n",
    "        movement_intensity = 0.02  # very calm\n",
    "    elif t < NUM_ROWS * 0.9:\n",
    "        movement_intensity = 0.08  # increasing\n",
    "    else:\n",
    "        movement_intensity = 0.4  # babies starting to break shells\n",
    "\n",
    "    ax = random.uniform(-movement_intensity, movement_intensity)\n",
    "    ay = random.uniform(-movement_intensity, movement_intensity)\n",
    "    az = 1 + random.uniform(-movement_intensity, movement_intensity)  # gravity reference\n",
    "\n",
    "    gx = random.uniform(-movement_intensity, movement_intensity)\n",
    "    gy = random.uniform(-movement_intensity, movement_intensity)\n",
    "    gz = random.uniform(-movement_intensity, movement_intensity)\n",
    "\n",
    "    return (\n",
    "        round(ax, 4), round(ay, 4), round(az, 4),\n",
    "        round(gx, 4), round(gy, 4), round(gz, 4)\n",
    "    )\n",
    "\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    for t in range(NUM_ROWS):\n",
    "        temp = generate_temperature(t)\n",
    "        ax, ay, az, gx, gy, gz = generate_movement(t)\n",
    "\n",
    "        reading = {\n",
    "            \"timestamp\": (start_time + datetime.timedelta(seconds=t)).isoformat(),\n",
    "            \"temperature_c\": temp,\n",
    "            \"accelerometer\": {\"ax\": ax, \"ay\": ay, \"az\": az},\n",
    "            \"gyroscope\": {\"gx\": gx, \"gy\": gy, \"gz\": gz}\n",
    "        }\n",
    "\n",
    "        f.write(json.dumps(reading) + \"\\n\")\n",
    "\n",
    "print(f\"Generated {NUM_ROWS} sensor readings in:\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be83e70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U dspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b126fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento do corpus\n",
    "\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "with open(\"./data/turtle_sensors.jsonl\") as f:\n",
    "    corpus = []\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        \n",
    "        text = (\n",
    "            f\"timestamp: {obj['timestamp']}. \"\n",
    "            f\"temperature: {obj['temperature_c']}C. \"\n",
    "            f\"accelerometer ax={obj['accelerometer']['ax']}, \"\n",
    "            f\"ay={obj['accelerometer']['ay']}, \"\n",
    "            f\"az={obj['accelerometer']['az']}. \"\n",
    "            f\"gyroscope gx={obj['gyroscope']['gx']}, \"\n",
    "            f\"gy={obj['gyroscope']['gy']}, \"\n",
    "            f\"gz={obj['gyroscope']['gz']}.\"\n",
    "        )\n",
    "        corpus.append(text)\n",
    "\n",
    "print(\"Loaded\", len(corpus), \"sensor readings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15218892",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dspy\n",
    "\n",
    "# cria√ß√£o do embedder + retriever\n",
    "model = SentenceTransformer(\"sentence-transformers/static-retrieval-mrl-en-v1\", device=\"cpu\")\n",
    "embedder = dspy.Embedder(model.encode)\n",
    "search = dspy.retrievers.Embeddings(embedder=embedder, corpus=corpus, k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588aacd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurando a LM\n",
    "\n",
    "# %pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "anth_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "print(\"Loaded:\", anth_api_key[:5] + \"...\" if anth_api_key else \"Not found\")\n",
    "\n",
    "lm = dspy.LM(\"gemini/gemini-2.5-flash-lite\", api_key=gemini_api_key)\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345efe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SensorRAG(dspy.Module):\n",
    "    def __init__(self):\n",
    "        self.answer = dspy.ChainOfThought(\n",
    "            \"context, question -> response\"\n",
    "        )\n",
    "\n",
    "    def forward(self, question):\n",
    "        retrieved = search(question).passages\n",
    "        return self.answer(context=retrieved, question=question)\n",
    "\n",
    "sensor_rag = SensorRAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3edb692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1a Pergunta\n",
    "question = \"Qual foi a temperatura m√©dia?\"\n",
    "rag_output = sensor_rag(question=question)\n",
    "llm_answer = rag_output.response\n",
    "\n",
    "# recuperar documentos do RAG corretamente\n",
    "retrieved_docs = search(question).passages   # <-- AGORA √â LISTA DE STRINGS\n",
    "\n",
    "print(\"\\nDocumentos recuperados:\", len(retrieved_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c18ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2a Pergunta\n",
    "sensor_rag(question=\"Quando ocorreram picos de atividade do aceler√¥metro?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ea6879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3a Pergunta\n",
    "sensor_rag(question=\"Qual foi a temperatura m√°xima registrada?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b8df23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install deepeval\n",
    "%pip install anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "29361a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieved docs: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-oss:120b-cloud </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">Ollama</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-oss:120b-cloud \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mOllama\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Precision Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-oss:120b-cloud </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">Ollama</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Precision Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-oss:120b-cloud \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mOllama\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Recall Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-oss:120b-cloud </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">Ollama</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Recall Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-oss:120b-cloud \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mOllama\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-oss:120b-cloud </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">Ollama</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-oss:120b-cloud \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mOllama\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-oss:120b-cloud (Ollama), reason: The score is 1.00 because the answer directly addressed the question with no irrelevant content., error: None)\n",
      "  - ‚úÖ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-oss:120b-cloud (Ollama), reason: The score is 1.00 because every node in the retrieval list is relevant‚Äîeach node (ranks 1‚Äë10) contains a temperature reading such as \"temperature: 27.77C\" and \"temperature: 24.59C\"‚Äîso there are no irrelevant nodes ranked above any relevant ones., error: None)\n",
      "  - ‚ùå Contextual Recall (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-oss:120b-cloud (Ollama), reason: The score is 0.00 because sentence 1 ('A temperatura m√©dia correta √© 21.721C') cannot be linked to any node in retrieval context; none of the retrieved nodes mention a temperature of 21.721C., error: None)\n",
      "  - ‚úÖ Contextual Relevancy (score: 0.5263157894736842, threshold: 0.5, strict: False, evaluation model: gpt-oss:120b-cloud (Ollama), reason: The score is 0.53 because the context includes useful temperature readings such as \"temperature: 27.77C\" and \"The temperature was 28.73¬∞C at timestamp 2025-12-06T01:58:30.778004\", but it is also filled with unrelated entries like \"accelerometer ax=-0.0552, ay=-0.0068, az=0.9429\" and timestamps that \"do not provide any information about temperature\", which dilutes its overall relevance., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Qual foi a temperatura m√©dia?\n",
      "  - actual output: A temperatura m√©dia foi de 22.80C.\n",
      "  - expected output: A temperatura m√©dia correta √© 21.721C\n",
      "  - context: None\n",
      "  - retrieval context: ['timestamp: 2025-12-06T01:51:38.778004. temperature: 27.77C. accelerometer ax=0.0302, ay=0.0453, az=0.9947. gyroscope gx=0.0335, gy=0.0587, gz=-0.0664.', 'timestamp: 2025-12-05T23:31:23.778004. temperature: 24.59C. accelerometer ax=0.0109, ay=-0.0097, az=0.9921. gyroscope gx=-0.0053, gy=-0.0003, gz=-0.0035.', 'timestamp: 2025-12-06T01:58:30.778004. temperature: 28.73C. accelerometer ax=0.3776, ay=-0.3282, az=1.2403. gyroscope gx=-0.3819, gy=0.1256, gz=0.3795.', 'timestamp: 2025-12-06T02:01:33.778004. temperature: 23.01C. accelerometer ax=0.187, ay=-0.0993, az=1.3158. gyroscope gx=0.2755, gy=0.0335, gz=-0.3752.', 'timestamp: 2025-12-06T01:38:38.778004. temperature: 18.95C. accelerometer ax=-0.0348, ay=0.0135, az=1.058. gyroscope gx=-0.0699, gy=-0.0488, gz=0.0038.', 'timestamp: 2025-12-06T01:56:35.778004. temperature: 25.82C. accelerometer ax=0.3585, ay=0.1767, az=0.9724. gyroscope gx=-0.354, gy=0.2463, gz=0.172.', 'timestamp: 2025-12-05T23:37:55.778004. temperature: 20.78C. accelerometer ax=0.0059, ay=0.0003, az=0.9935. gyroscope gx=-0.0179, gy=-0.0087, gz=0.0048.', 'timestamp: 2025-12-06T01:47:35.778004. temperature: 18.35C. accelerometer ax=-0.0552, ay=-0.0068, az=0.9429. gyroscope gx=-0.0534, gy=-0.0335, gz=0.069.', 'timestamp: 2025-12-06T02:08:26.778004. temperature: 20.71C. accelerometer ax=0.0863, ay=0.3738, az=0.9737. gyroscope gx=-0.371, gy=0.0998, gz=-0.0638.', 'timestamp: 2025-12-06T02:08:15.778004. temperature: 19.3C. accelerometer ax=-0.379, ay=-0.0848, az=0.9287. gyroscope gx=0.0905, gy=0.0975, gz=-0.1589.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "Contextual Precision: 100.00% pass rate\n",
      "Contextual Recall: 0.00% pass rate\n",
      "Contextual Relevancy: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">‚ö† WARNING:</span> No hyperparameters logged.\n",
       "¬ª <a href=\"https://deepeval.com/docs/evaluation-prompts\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Log hyperparameters</span></a> to attribute prompts and models to your test runs.\n",
       "\n",
       "================================================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;33m‚ö† WARNING:\u001b[0m No hyperparameters logged.\n",
       "¬ª \u001b]8;id=567096;https://deepeval.com/docs/evaluation-prompts\u001b\\\u001b[1;34mLog hyperparameters\u001b[0m\u001b]8;;\u001b\\ to attribute prompts and models to your test runs.\n",
       "\n",
       "================================================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Evaluation completed üéâ! <span style=\"font-weight: bold\">(</span>time taken: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17.</span>33s | token cost: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span> USD<span style=\"font-weight: bold\">)</span>\n",
       "¬ª Test Results <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> total tests<span style=\"font-weight: bold\">)</span>:\n",
       "   ¬ª Pass Rate: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>% | Passed: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0</span> | Failed: <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">1</span>\n",
       "\n",
       " ================================================================================ \n",
       "\n",
       "¬ª Want to share evals with your team, or a place for your test cases to live? ‚ù§Ô∏è üè°\n",
       "  ¬ª Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval view'</span> to analyze and save testing results on <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span>.\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n",
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Evaluation completed üéâ! \u001b[1m(\u001b[0mtime taken: \u001b[1;36m17.\u001b[0m33s | token cost: \u001b[1;36m0.0\u001b[0m USD\u001b[1m)\u001b[0m\n",
       "¬ª Test Results \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m total tests\u001b[1m)\u001b[0m:\n",
       "   ¬ª Pass Rate: \u001b[1;36m0.0\u001b[0m% | Passed: \u001b[1;32m0\u001b[0m | Failed: \u001b[1;31m1\u001b[0m\n",
       "\n",
       " ================================================================================ \n",
       "\n",
       "¬ª Want to share evals with your team, or a place for your test cases to live? ‚ù§Ô∏è üè°\n",
       "  ¬ª Run \u001b[1;32m'deepeval view'\u001b[0m to analyze and save testing results on \u001b[38;2;106;0;255mConfident AI\u001b[0m.\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RESULTADOS ---\n",
      "('test_results', [TestResult(name='test_case_0', success=False, metrics_data=[MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the answer directly addressed the question with no irrelevant content.', strict_mode=False, evaluation_model='gpt-oss:120b-cloud (Ollama)', error=None, evaluation_cost=0.0, verbose_logs='Statements:\\n[\\n    \"A temperatura m√©dia foi de 22.80¬∞C.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]'), MetricData(name='Contextual Precision', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because every node in the retrieval list is relevant‚Äîeach node (ranks 1‚Äë10) contains a temperature reading such as \"temperature: 27.77C\" and \"temperature: 24.59C\"‚Äîso there are no irrelevant nodes ranked above any relevant ones.', strict_mode=False, evaluation_model='gpt-oss:120b-cloud (Ollama)', error=None, evaluation_cost=0.0, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The document provides a temperature reading: \\'temperature: 27.77C\\', which can be used in calculating the average.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The document provides a temperature reading: \\'temperature: 24.59C\\', which can be used in calculating the average.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The document provides a temperature reading: \\'temperature: 28.73C\\', which can be used in calculating the average.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The document provides a temperature reading: \\'temperature: 23.01C\\', which can be used in calculating the average.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The document provides a temperature reading: \\'temperature: 18.95C\\', which can be used in calculating the average.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The document provides a temperature reading: \\'temperature: 25.82C\\', which can be used in calculating the average.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The document provides a temperature reading: \\'temperature: 20.78C\\', which can be used in calculating the average.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The document provides a temperature reading: \\'temperature: 18.35C\\', which can be used in calculating the average.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The document provides a temperature reading: \\'temperature: 20.71C\\', which can be used in calculating the average.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The document provides a temperature reading: \\'temperature: 19.3C\\', which can be used in calculating the average.\"\\n    }\\n]'), MetricData(name='Contextual Recall', threshold=0.5, success=False, score=0.0, reason=\"The score is 0.00 because sentence 1 ('A temperatura m√©dia correta √© 21.721C') cannot be linked to any node in retrieval context; none of the retrieved nodes mention a temperature of 21.721C.\", strict_mode=False, evaluation_model='gpt-oss:120b-cloud (Ollama)', error=None, evaluation_cost=0.0, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"None of the retrieved nodes report a temperature of 21.721C \\\\u2013 they list values like 27.77C, 24.59C, 28.73C, etc., so the sentence cannot be sourced from the context.\"\\n    }\\n]'), MetricData(name='Contextual Relevancy', threshold=0.5, success=True, score=0.5263157894736842, reason='The score is 0.53 because the context includes useful temperature readings such as \"temperature: 27.77C\" and \"The temperature was 28.73¬∞C at timestamp 2025-12-06T01:58:30.778004\", but it is also filled with unrelated entries like \"accelerometer ax=-0.0552, ay=-0.0068, az=0.9429\" and timestamps that \"do not provide any information about temperature\", which dilutes its overall relevance.', strict_mode=False, evaluation_model='gpt-oss:120b-cloud (Ollama)', error=None, evaluation_cost=0.0, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"temperature: 27.77C\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            }\\n        ]\\n    },\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"timestamp: 2025-12-05T23:31:23.778004. temperature: 24.59C. accelerometer ax=0.0109, ay=-0.0097, az=0.9921. gyroscope gx=-0.0053, gy=-0.0003, gz=-0.0035.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            }\\n        ]\\n    },\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"The temperature was 28.73\\\\u00b0C at timestamp 2025-12-06T01:58:30.778004.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            }\\n        ]\\n    },\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"timestamp: 2025-12-06T02:01:33.778004. temperature: 23.01C. accelerometer ax=0.187, ay=-0.0993, az=1.3158. gyroscope gx=0.2755, gy=0.0335, gz=-0.3752.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            }\\n        ]\\n    },\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"temperature: 18.95C\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            }\\n        ]\\n    },\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"temperature: 25.82C\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            }\\n        ]\\n    },\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"The temperature was 20.78\\\\u00b0C.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            }\\n        ]\\n    },\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"timestamp: 2025-12-06T01:47:35.778004\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'timestamp: 2025-12-06T01:47:35.778004\\' does not provide any information about temperature, which is required for answering the question about the average temperature.\"\\n            },\\n            {\\n                \"statement\": \"temperature: 18.35C\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"accelerometer ax=-0.0552, ay=-0.0068, az=0.9429\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'accelerometer ax=-0.0552, ay=-0.0068, az=0.9429\\' pertains to motion data, not to temperature, so it is irrelevant to the question.\"\\n            },\\n            {\\n                \"statement\": \"gyroscope gx=-0.0534, gy=-0.0335, gz=0.069\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'gyroscope gx=-0.0534, gy=-0.0335, gz=0.069\\' describes rotation data, which does not relate to temperature.\"\\n            }\\n        ]\\n    },\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"timestamp: 2025-12-06T02:08:26.778004\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The timestamp \\'2025-12-06T02:08:26.778004\\' does not provide information about the temperature, which is what the question asks.\"\\n            },\\n            {\\n                \"statement\": \"temperature: 20.71C\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"accelerometer ax=0.0863, ay=0.3738, az=0.9737\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The accelerometer readings \\'ax=0.0863, ay=0.3738, az=0.9737\\' are unrelated to the temperature query.\"\\n            },\\n            {\\n                \"statement\": \"gyroscope gx=-0.371, gy=0.0998, gz=-0.0638\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The gyroscope data \\'gx=-0.371, gy=0.0998, gz=-0.0638\\' does not address the temperature information requested.\"\\n            }\\n        ]\\n    },\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"timestamp: 2025-12-06T02:08:15.778004\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\\\\"timestamp: 2025-12-06T02:08:15.778004\\\\\" provides only a time reference and does not contain any information about temperature.\"\\n            },\\n            {\\n                \"statement\": \"temperature: 19.3C\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"accelerometer ax=-0.379, ay=-0.0848, az=0.9287\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\\\\"accelerometer ax=-0.379, ay=-0.0848, az=0.9287\\\\\" describes motion data, which is unrelated to temperature.\"\\n            },\\n            {\\n                \"statement\": \"gyroscope gx=0.0905, gy=0.0975, gz=-0.1589\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\\\\"gyroscope gx=0.0905, gy=0.0975, gz=-0.1589\\\\\" reports rotation data, which has no relevance to temperature.\"\\n            }\\n        ]\\n    }\\n]')], conversational=False, multimodal=False, input='Qual foi a temperatura m√©dia?', actual_output='A temperatura m√©dia foi de 22.80C.', expected_output='A temperatura m√©dia correta √© 21.721C', context=None, retrieval_context=['timestamp: 2025-12-06T01:51:38.778004. temperature: 27.77C. accelerometer ax=0.0302, ay=0.0453, az=0.9947. gyroscope gx=0.0335, gy=0.0587, gz=-0.0664.', 'timestamp: 2025-12-05T23:31:23.778004. temperature: 24.59C. accelerometer ax=0.0109, ay=-0.0097, az=0.9921. gyroscope gx=-0.0053, gy=-0.0003, gz=-0.0035.', 'timestamp: 2025-12-06T01:58:30.778004. temperature: 28.73C. accelerometer ax=0.3776, ay=-0.3282, az=1.2403. gyroscope gx=-0.3819, gy=0.1256, gz=0.3795.', 'timestamp: 2025-12-06T02:01:33.778004. temperature: 23.01C. accelerometer ax=0.187, ay=-0.0993, az=1.3158. gyroscope gx=0.2755, gy=0.0335, gz=-0.3752.', 'timestamp: 2025-12-06T01:38:38.778004. temperature: 18.95C. accelerometer ax=-0.0348, ay=0.0135, az=1.058. gyroscope gx=-0.0699, gy=-0.0488, gz=0.0038.', 'timestamp: 2025-12-06T01:56:35.778004. temperature: 25.82C. accelerometer ax=0.3585, ay=0.1767, az=0.9724. gyroscope gx=-0.354, gy=0.2463, gz=0.172.', 'timestamp: 2025-12-05T23:37:55.778004. temperature: 20.78C. accelerometer ax=0.0059, ay=0.0003, az=0.9935. gyroscope gx=-0.0179, gy=-0.0087, gz=0.0048.', 'timestamp: 2025-12-06T01:47:35.778004. temperature: 18.35C. accelerometer ax=-0.0552, ay=-0.0068, az=0.9429. gyroscope gx=-0.0534, gy=-0.0335, gz=0.069.', 'timestamp: 2025-12-06T02:08:26.778004. temperature: 20.71C. accelerometer ax=0.0863, ay=0.3738, az=0.9737. gyroscope gx=-0.371, gy=0.0998, gz=-0.0638.', 'timestamp: 2025-12-06T02:08:15.778004. temperature: 19.3C. accelerometer ax=-0.379, ay=-0.0848, az=0.9287. gyroscope gx=0.0905, gy=0.0975, gz=-0.1589.'], turns=None, additional_metadata=None)])\n",
      "('confident_link', None)\n",
      "('test_run_id', None)\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import (\n",
    "    ContextualPrecisionMetric,\n",
    "    ContextualRecallMetric,\n",
    "    ContextualRelevancyMetric,\n",
    "    AnswerRelevancyMetric\n",
    ")\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval import evaluate\n",
    "from deepeval.models import GeminiModel, AnthropicModel, OllamaModel\n",
    "import re\n",
    "\n",
    "def rag_pipeline(question):\n",
    "    rag_output = sensor_rag(question=question)\n",
    "    llm_answer = rag_output.response\n",
    "    retrieved_docs = search(question).passages\n",
    "    print(\"\\nRetrieved docs:\", len(retrieved_docs))\n",
    "    return llm_answer, retrieved_docs\n",
    "\n",
    "# ---- PERGUNTA ----\n",
    "question = \"Qual foi a temperatura m√©dia?\"\n",
    "\n",
    "# ---- CALCULAR A RESPOSTA ESPERADA (GROUND TRUTH) ----\n",
    "temps = []\n",
    "for text in corpus:\n",
    "    m = re.search(r\"temperature: ([0-9.]+)C\", text)\n",
    "    if m:\n",
    "        temps.append(float(m.group(1)))\n",
    "\n",
    "expected_output = f\"A temperatura m√©dia correta √© {sum(temps)/len(temps):.3f}C\"\n",
    "\n",
    "# Cria√ß√£o dos casos de teste\n",
    "saida_llm_1, contextos_saida = rag_pipeline(question)\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=question,\n",
    "    actual_output=saida_llm_1,\n",
    "    retrieval_context=contextos_saida,\n",
    "    expected_output=expected_output\n",
    ")\n",
    "\n",
    "# ---- MODELO OPEN-SOURCE PARA AVALIA√á√ÉO ----\n",
    "# eval_model = DeepEvalOllamaModel(model=\"llama3.1\")\n",
    "eval_model = OllamaModel(\"gpt-oss:120b-cloud\")\n",
    "\n",
    "# ---- M√âTRICAS ----\n",
    "answer_relevancy = AnswerRelevancyMetric(model=eval_model)\n",
    "contextual_precision = ContextualPrecisionMetric(model=eval_model)\n",
    "contextual_recall = ContextualRecallMetric(model=eval_model)\n",
    "contextual_relevancy = ContextualRelevancyMetric(model=eval_model)\n",
    "\n",
    "# ---- EXECUTAR AVALIA√á√ÉO ----\n",
    "results = evaluate(\n",
    "    test_cases=[test_case],\n",
    "    metrics=[answer_relevancy, contextual_precision, contextual_recall, contextual_relevancy]\n",
    ")\n",
    "\n",
    "# ---- RESULTADOS ----\n",
    "print(\"\\n--- RESULTADOS ---\")\n",
    "for r in results:\n",
    "    print(r)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
